
<h2>Transformer Papers</h2>


<ul>

                             

 <li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(1).pdf" style="text-decoration:none;">Attention Is All You Need</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(2).pdf" style="text-decoration:none;">Weighted Transformer Network for Machine Translation</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(3).pdf" style="text-decoration:none;">Self-Attention with Relative Position Representations</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(4).pdf" style="text-decoration:none;">Accelerating Neural Transformer via an Average Attention Network</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(5).pdf" style="text-decoration:none;">Universal Transformers</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(6).pdf" style="text-decoration:none;">Transformer-XL: Attentive Language Models
Beyond a Fixed-Length Context</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(7).pdf" style="text-decoration:none;">Star-Transformer</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(8).pdf" style="text-decoration:none;"> Generating Long Sequences with Sparse Transformers </a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(9).pdf" style="text-decoration:none;">Analyzing Multi-Head Self-Attention:
Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned</a></li>
  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(10).pdf" style="text-decoration:none;">Transformer Dissection: A Unified Understanding of Transformer's Attention via the Lens of Kernel </a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(11).pdf" style="text-decoration:none;">Transformers without Tears:
Improving the Normalization of Self-Attention</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(12).pdf" style="text-decoration:none;">Improving Transformer Models by Reordering their Sublayers</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(13).pdf" style="text-decoration:none;">TENER: Adapting Transformer Encoder for Named Entity Recognition</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(14).pdf" style="text-decoration:none;">Compressive Transformers for Long-Range Sequence Modelling</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(15).pdf" style="text-decoration:none;">Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(16).pdf" style="text-decoration:none;">Reformer: The Efficient Transformer</a></li>

  <li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(17).pdf" style="text-decoration:none;">On Layer Normalization in the Transformer Architecture</a></li>   
  
<li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(18).pdf" style="text-decoration:none;">Talking-Heads Attention</a></li> 

  
<li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(19).pdf" style="text-decoration:none;">ReZero is All You Need:
Fast Convergence at Large Depth</a></li> 

<li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(20).pdf" style="text-decoration:none;">Longformer: The Long-Document Transformer</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(21).pdf" style="text-decoration:none;">Highway Transformer: Self-Gating Enhanced Self-Attentive Networks</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(22).pdf" style="text-decoration:none;">Lite Transformer with Long-Short Range Attention</a></li> 
 <li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(23).pdf" style="text-decoration:none;">Synthesizer: Rethinking Self-Attention in Transformer Models</a></li> 
 

   <li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(24).pdf" style="text-decoration:none;">HAT: Hardware-Aware Transformers for
Efficient Natural Language Processing</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(25).pdf" style="text-decoration:none;">Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing</a></li>                              
 <li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(26).pdf" style="text-decoration:none;">Linformer: Self-Attention with Linear Complexity</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(27).pdf" style="text-decoration:none;">Memory Transformer</a></li>
   
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(28).pdf" style="text-decoration:none;">Transformers are RNNs:
Fast Autoregressive Transformers with Linear Attention</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(29).pdf" style="text-decoration:none;">Multi-Head Attention:
Collaborate Instead of Concatenate </a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(30).pdf" style="text-decoration:none;">Fast Transformers with Clustered Attention</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(31).pdf" style="text-decoration:none;">Big Bird: Transformers for Longer Sequences</a></li> 
    <li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(32).pdf" style="text-decoration:none;">Efficient Transformers: A Survey</a></li> 

   <li><a target="_blank" href="https://github.com/manjunath5496/Transformer-Papers/blob/master/tram(33).pdf" style="text-decoration:none;">Memory Transformer Networks</a></li>                              

  </ul>
  
  
  
